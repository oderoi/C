{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will:\n",
    "\n",
    " * Update gradient descent for logistic regression.\n",
    " * See gradient descent on a familiar data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the gradient descent algorithm utilizes the gradient calculation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "&\\text{repeat until convergence:} \\; \\lbrace \\\\\n",
    "&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1} \\\\ \n",
    "&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n",
    "&\\rbrace\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where each iteration performs simultaneous updates on $w_j$ for all $j$, where\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{2} \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3} \n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* m is the number of training examples in the data set      \n",
    "* $f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target\n",
    "* For a logistic regression model  \n",
    "    $z = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n",
    "    $f_{\\mathbf{w},b}(x) = g(z)$  \n",
    "    where $g(z)$ is the sigmoid function:  \n",
    "    $g(z) = \\frac{1}{1+e^{-z}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the Gradient, Code Description\n",
    "Implements equation (2),(3) above for all $w_j$ and $b$.\n",
    "There are many ways to implement this. Outlined below is this:\n",
    "- initialize variables to accumulate `dj_dw` and `dj_db`\n",
    "- for each example\n",
    "    - calculate the error for that example $g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b) - \\mathbf{y}^{(i)}$\n",
    "    - for each input value $x_{j}^{(i)}$ in this example,  \n",
    "        - multiply the error by the input  $x_{j}^{(i)}$, and add to the corresponding element of `dj_dw`. (equation 2 above)\n",
    "    - add the error to `dj_db` (equation 3 above)\n",
    "\n",
    "- divide `dj_db` and `dj_dw` by total number of examples (m)\n",
    "- note that $\\mathbf{x}^{(i)}$ in numpy `X[i,:]` or `X[i]`  and $x_{j}^{(i)}$ is `X[i,j]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python - imports**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C - imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math, copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python - sigmoid**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C - sigmoid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    compute sigmoid\n",
    "\n",
    "    Args:\n",
    "        z (scalar):  logistic function, f_wb\n",
    "\n",
    "    Return:\n",
    "        g (scalar):  sigmoid of f_wb\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python - gradient**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C - gradient**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_logistic(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression\n",
    "\n",
    "    Args:\n",
    "        X (adarray (m,n)):  Data, m examples with n features\n",
    "        y (ndarray (m, )):  target values\n",
    "        w (ndarray (n, )):  model parameters\n",
    "        b (scalar)       :  model parameter\n",
    "\n",
    "    Returns:\n",
    "        dj_dw (ndarray (n, )):  The gradient of the cost w.r.t the parameters w.\n",
    "        dj_db (scalar)       :  The gradient of the cost w.r.t the parameter b.\n",
    "    \"\"\"\n",
    "\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))              #(n,)\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i], w) + b)           #(m,n)(n,)=scalar\n",
    "        err_i = f_wb_i - y[i]                           #scalar\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]        #scalar\n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                     #(n,)\n",
    "    dj_db = dj_db/m                                     #scalar\n",
    "\n",
    "    return dj_dw,dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python - cost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C - cost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logistic(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Compute cost\n",
    "    Args:\n",
    "        X (ndarray (m,n)):  input data \n",
    "        y (ndarray (m,)):   output data\n",
    "        w (ndarray (n,)):   model parametrs \n",
    "        b (scalar):         model parameter\n",
    "\n",
    "    Return:\n",
    "        J (sclar):  Cost of logistic model\n",
    "    \"\"\"\n",
    "\n",
    "    J = 0.0\n",
    "    f_wb_i = 0.0\n",
    "    m = X.shape[0]\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i], w) + b)                             #(m,): scalar\n",
    "        J += -y[i]*np.log(f_wb_i) - (1 - y[i])*np.log(1 - f_wb_i)   \n",
    "    J /= m\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python - main**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C - main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_dw: [0.49833339 0.49883943]\n",
      "dj_db: 0.49861806546328574\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    X_tmp = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "    y_tmp = np.array([0, 0, 0, 1, 1, 1])\n",
    "    w_tmp = np.array([2.,3.])\n",
    "    b_tmp = 1.\n",
    "\n",
    "    dj_dw_tmp, dj_db_tmp = compute_gradient_logistic(X_tmp, y_tmp, w_tmp, b_tmp)\n",
    "\n",
    "    print(f\"dj_dw: {dj_dw_tmp}\")\n",
    "    print(f\"dj_db: {dj_db_tmp}\")\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python - Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C - Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iter):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent\n",
    "    \n",
    "     Args:\n",
    "      X (ndarray (m,n)   : Data, m examples with n features\n",
    "      y (ndarray (m,))   : target values\n",
    "      w_in (ndarray (n,)): Initial values of model parameters  \n",
    "      b_in (scalar)      : Initial values of model parameter\n",
    "      alpha (float)      : Learning rate\n",
    "      num_iters (scalar) : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,))   : Updated values of parameters\n",
    "      b (scalar)         : Updated value of parameter \n",
    "    \"\"\"\n",
    "\n",
    "    #An array to store cost J and w's each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)     #avoid modifying global w within function\n",
    "    b = b_in\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        #calculate gradient and update the parameters\n",
    "        dj_dw, dj_db = compute_gradient_logistic(X, y, w, b)\n",
    "\n",
    "        #Update Parameters using w, b, alpha and gardient\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "\n",
    "        #Save cost J each iteration\n",
    "        if i<100000:          #prevent resource exhaustion\n",
    "            J_history.append(compute_cost_logistic(X, y, w, b))\n",
    "\n",
    "        #Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iter / 10) ==0:\n",
    "          print(f\"Iteration {i:4d}  Cost {J_history[-1]}  \")\n",
    "\n",
    "    return w, b, J_history                 #return final w, b and J history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0  Cost 0.684610468560574  \n",
      "Iteration 10000  Cost 0.01711604647887364  \n",
      "Iteration 20000  Cost 0.008523403979166467  \n",
      "Iteration 30000  Cost 0.005672197191107633  \n",
      "Iteration 40000  Cost 0.004250161053834308  \n",
      "Iteration 50000  Cost 0.003398230224179212  \n",
      "Iteration 60000  Cost 0.0028308425601004327  \n",
      "Iteration 70000  Cost 0.002425848306579758  \n",
      "Iteration 80000  Cost 0.0021222573122028584  \n",
      "Iteration 90000  Cost 0.0018862216652143864  \n",
      "\n",
      "updated parameters: w:[8.35313087 8.15226727],    b:-22.690605796630248\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "    y_train = np.array([0, 0, 0, 1, 1, 1])\n",
    "    \n",
    "    w_tmp  = np.zeros_like(X_train[0])\n",
    "    b_tmp  = 0.\n",
    "    alph = 0.1\n",
    "    iters = 100000\n",
    "\n",
    "    w_out, b_out, _=gradient_descent(X_train, y_train, w_tmp, b_tmp, alph, iters)\n",
    "    print(f\"\\nupdated parameters: w:{w_out},    b:{b_out}\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
