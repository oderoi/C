{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Variable Linear Regression\n",
    "\n",
    "#### 1.0 Problem Statement\n",
    "\n",
    "We will use the motivating example of housing price prediction. The training dataset contains four features (size, bedrooms, floors and, age) shown in the table below.\n",
    "\n",
    "| Size (sqft)| Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  |   \n",
    "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
    "| 2104            | 5                   | 1                | 45           | 460           |  \n",
    "| 1416            | 3                   | 2                | 40           | 232           |  \n",
    "| 852             | 2                   | 1                | 35           | 178           |  \n",
    "\n",
    "Let's build a linear regression model using these values so you can then predict the price for other houses. For example, a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old.  \n",
    "\n",
    "\n",
    "_**1.1 Matrix $X$ containing our example**_ \n",
    "\n",
    "Each row of datasets represent one example, with $n$ number of features and $m$ training examples, now $\\mathbf{x}$ is an input matrix with dimensions $(m, n)$ where $m$ is row and $n$ is column.\n",
    "\n",
    "$$\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    " x^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\ \n",
    " x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n",
    " \\vdots  &  \\vdots  &  \\vdots  &  \\vdots  \\\\\n",
    " x^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "notation:\n",
    "- $\\mathbf{x}^{(i)}$ is vector containing example i. $\\mathbf{x}^{(i)}$ $ = (x^{(i)}_0, x^{(i)}_1, \\cdots,x^{(i)}_{n-1})$\n",
    "- $x^{(i)}_j$ is element j in example i. The superscript in parenthesis indicates the example number while the subscript represents an element. \n",
    "\n",
    "\n",
    "_**1.2 Parameter vector $\\mathbf{w}_{j}$ , b**_\n",
    "\n",
    "* $\\mathbf{w}$ is a vector with $n$ elements.\n",
    "  - Each element contains the parameter associated with one feature.\n",
    "  - in our dataset, n is 4.\n",
    "  - notionally, we draw this as a column vector\n",
    "\n",
    "$$\\mathbf{w} = \\begin{pmatrix}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\\n",
    "\\vdots\\\\\n",
    "w_{n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "* $b$ is a scalar parameter. \n",
    "\n",
    "#### 2.0 Load datasets\n",
    "\n",
    "As shown in a problem statement, our dataset contains five features (size, bedrooms, floors and, age, price of the house), $n = 5$ and ahundred examples, $m = 100$.\n",
    "\n",
    "In this case our dataset will have the size of $(m , n) = (100 , 5)$\n",
    "\n",
    "$$\\mathbf{data} = \n",
    "\\begin{pmatrix}\n",
    " x^{(0)}_{0} & x^{(0)}_{1} & x^{(0)}_{2} & \\cdots & y^{(0)}_{n-1} \\\\ \n",
    " x^{(1)}_{0} & x^{(1)}_{1} & x^{(1)}_{2} & \\cdots & y^{(1)}_{n-1} \\\\\n",
    " \\vdots      & \\vdots      & \\vdots      & \\vdots      & \\vdots\\\\\n",
    " x^{(m-1)}_{0} & x^{(m-1)}_{1} & x^{(m-1)}_{2} & \\cdots & y^{(m-1)}_{n-1}\n",
    "\\end{pmatrix}\n",
    "\n",
    "=\n",
    " \n",
    "\\begin{pmatrix}\n",
    " x^{(0)}_{0} & x^{(0)}_{1} & x^{(0)}_{2} & x^{(0)}_{3} & y^{(0)}_{4} \\\\ \n",
    " x^{(1)}_{0} & x^{(1)}_{1} & x^{(1)}_{2} & x^{(1)}_{3} & y^{(1)}_{4} \\\\\n",
    " \\vdots      & \\vdots      & \\vdots      & \\vdots      & \\vdots\\\\\n",
    " x^{(99)}_{0} & x^{(99)}_{1} & x^{(99)}_{2} & x^{(99)}_{3} & y^{(99)}_{4}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$data \\in \\mathbb{R}_{m \\times n}  =  \\mathbb{R}_{100 \\times 5}$$\n",
    "\n",
    "_**2.1 Load input datasets, $\\mathbf{x}^{(i)}_{j}$**_  \n",
    "\n",
    "Input datasets will comprise of five features (size, bedrooms, floors and, age), $n = 4$ and hundred exaples, $m = 100$.\n",
    "\n",
    "$$\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    " x^{(0)}_{0} & x^{(0)}_{1} & \\cdots & x^{(0)}_{n-1}  \\\\ \n",
    " x^{(1)}_{0} & x^{(1)}_{1} & \\cdots & x^{(1)}_{n-1}  \\\\\n",
    " \\vdots      & \\vdots      & \\vdots      & \\vdots \\\\\n",
    " x^{(m-1)}_{0} & x^{(m-1)}_{1} & \\cdots & x^{(m-1)}_{n-1} \n",
    "\\end{pmatrix}\n",
    " = \n",
    "\\begin{pmatrix}\n",
    " x^{(0)}_{0} & x^{(0)}_{1} & x^{(0)}_{2} & x^{(0)}_{3}  \\\\ \n",
    " x^{(1)}_{0} & x^{(1)}_{1} & x^{(1)}_{2} & x^{(1)}_{3}  \\\\\n",
    " \\vdots      & \\vdots      & \\vdots      & \\vdots \\\\\n",
    " x^{(99)}_{0} & x^{(99)}_{1} & x^{(99)}_{2} & x^{(99)}_{3} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\\mathbf{X} \\in \\mathbb{R}_{m \\times n}  =  \\mathbb{R}_{100 \\times 4}$$\n",
    "\n",
    "_**2.2 Load output datasets, $\\mathbf{y}^{(i)}_{j}$**_  \n",
    "\n",
    "Input datasets will comprise of only one features (price), $n = 1$ and hundred exaples, $m = 100$.\n",
    "\n",
    "$$\\mathbf{Y} = \n",
    "\\begin{pmatrix}\n",
    "  y^{(0)}_{0} \\\\ \n",
    "  y^{(1)}_{0} \\\\\n",
    "  \\vdots     \\\\\n",
    " y^{(m-1)}_{0}\n",
    "\\end{pmatrix}\n",
    " = \n",
    "\\begin{pmatrix}\n",
    "  y^{(0)}_{0} \\\\ \n",
    "  y^{(1)}_{0} \\\\\n",
    "  \\vdots     \\\\\n",
    " y^{(99)}_{0}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$Y \\in \\mathbb{R}_{m \\times n}  =  \\mathbb{R}_{100 \\times 1}$$\n",
    "\n",
    "\n",
    "_**2.3 Initialize parameters  $\\mathbf{w}_{j}$  , b**_\n",
    "\n",
    "* $\\mathbf{w}_{j}$ is a vector with $n = 4$ elements.\n",
    "  - Each element contains the parameter associated with one feature.\n",
    "\n",
    "$$\\mathbf{w} = \\begin{pmatrix}\n",
    "w_{0} \\\\ \n",
    "w_{1} \\\\\n",
    "\\vdots\\\\\n",
    "w_{n-1}\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "w_{0} \\\\ \n",
    "w_{1} \\\\\n",
    "w_{2}\\\\\n",
    "w_{3}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\\mathbf{w} \\in \\mathbb{R}_{n \\times 1}  =  \\mathbb{R}_{4 \\times 1}$$\n",
    "\n",
    "* $b$ is a scalar parameter. \n",
    "\n",
    "#### 3.0 Model Prediction With Multiple Variables\n",
    "The model's prediction with multiple variables is given by the linear model:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "or in vector notation:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{X} + \\mathbf{b}  \\tag{2} $$ \n",
    "\n",
    "$$f_{\\mathbf{w},b}(\\mathbf{x}) \\in \\mathbb{R}_{m \\times 1}  =  \\mathbf{w} \\in \\mathbb{R}_{n \\times 1} \\cdot \\mathbf{X} \\in \\mathbb{R}_{m \\times n} + \\mathbf{b} \\in \\mathbb{R}_{1}$$\n",
    "\n",
    "$$f_{\\mathbf{w},b}(\\mathbf{x})_{m \\times 1}  =  \\mathbf{w}_{n \\times 1} \\cdot \\mathbf{X}_{m \\times n} + \\mathbf{b}_{1}$$\n",
    "\n",
    "$$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) \\in \\mathbb{R}_{m \\times 1}$$\n",
    "\n",
    "$$f_{\\mathbf{w},b}(\\mathbf{x}) = \n",
    "\\begin{pmatrix}\n",
    "  f_{\\mathbf{w},b}(\\mathbf{x})^{(0)}_{0} \\\\ \n",
    "  f_{\\mathbf{w},b}(\\mathbf{x})^{(1)}_{0} \\\\\n",
    "  \\vdots     \\\\\n",
    " f_{\\mathbf{w},b}(\\mathbf{x})^{(m-1)}_{0}\n",
    "\\end{pmatrix}\n",
    " = \n",
    "\\begin{pmatrix}\n",
    "  f_{\\mathbf{w},b}(\\mathbf{x})^{(0)}_{0} \\\\ \n",
    "  f_{\\mathbf{w},b}(\\mathbf{x})^{(1)}_{0} \\\\\n",
    "  \\vdots     \\\\\n",
    " f_{\\mathbf{w},b}(\\mathbf{x})^{(99)}_{0}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "where $\\cdot$ is a vector `dot product`\n",
    "\n",
    "To demonstrate the dot product, we will implement prediction using (1) and (2).\n",
    " \n",
    "\n",
    "#### 4.0 Compute Cost With Multiple Variables\n",
    "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{X} + b  \\tag{4} $$ \n",
    "\n",
    "$$f_{\\mathbf{w},b}(\\mathbf{x}) \\in \\mathbb{R}_{m \\times 1}$$\n",
    "\n",
    "\n",
    "In contrast to previous labs, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features.\n",
    "\n",
    "\n",
    "#### 5.0 Gradient Descent With Multiple Variables\n",
    "Gradient descent for multiple variables:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "* m is the number of training examples in the data set\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value\n",
    "\n",
    "Note: \n",
    "\n",
    "*  $$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\in \\mathbb{R}_{1 \\times n} = \\mathbb{R}_{1 \\times 4}$$\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}\n",
    "= \n",
    "\\begin{pmatrix}\n",
    "  \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_0} ,\n",
    "  \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_1} ,\n",
    "  \\cdots ,\n",
    " \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_{n-1}}\n",
    "\\end{pmatrix}\n",
    "= \n",
    "\\begin{pmatrix}\n",
    "  \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_0} ,\n",
    "  \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_1} ,\n",
    "  \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_2} ,\n",
    " \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_3}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "*  $$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\in \\mathbb{R}_{1}$$\n",
    "\n",
    "#### 6.0 Evaluating our model by compute R-squared\n",
    "\n",
    "R-squared is the proportion of the variance in the dependent variable that is explained by the independent variables in a regression model. It ranges from 0 to 1, where 0 means no relationship and 1 means a perfect fit\n",
    "\n",
    "R-squared(coefficient of Determination) tells us How well our model is  performing or how well our model's predictions match the real results. A higher R-squared means our model is doing a better job predicting.\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{Residual\\ Sum\\ of\\ Squares\\ (\\mathbf{ss}_{res})}{Total\\ Sum\\ of\\ Squares\\ (\\mathbf{ss}_{total})}\n",
    "$$\n",
    "\n",
    "![Residual Sum of Squares](images/ssr.png)\n",
    "\n",
    "![Total Sum of Squares](images/sst.png)\n",
    "\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\mathbf{ss}_{res}}{\\mathbf{ss}_{total}}\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum\\limits_{i=0}^{(m-1)}(f_{w,b}(x^{(i)}) - y^{i})^2}{\\sum\\limits_{i=0}^{(m-1)}(y_{mean}^{(i)} - y^{(i)})^2}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
