{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization to Reduce Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will:\n",
    "- Understand what is overfit and how to reduce it.\n",
    "\n",
    "- Understand the concept of regularization and how it is used to reduce overfit.\n",
    "\n",
    "- Extend the previous linear and logistic cost function with a regularization term.\n",
    "\n",
    "- Extend the previous linear and logistic gradients with a regularization term added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To understand overfitting let's say:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a training set with $m$ examples and $n$ features, we want to predict the output in weither linear or logistic prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If our predictions from our model ***does not*** fit the training set ***well***:\n",
    "\n",
    "    * Here we will say our model is **Underfitting** the data or has **high bias**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. If our predictios from our model ***fits*** the training set ***just well***.\n",
    "\n",
    "    * Here we will say our model is **Generalized**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. If our predictions from our model ***fits*** the training set ***extremely well***.\n",
    "\n",
    "    * Here we will say our model is ***Overfit*** the data or has ***high variance***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **How to address Overfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Collect more training data.\n",
    "\n",
    "    * If you can get more dataset, you can add them to the training set.\n",
    "\n",
    "2. Reduce number of features, $n$.\n",
    "\n",
    "    * Perform **Feature sellection**.\n",
    "    \n",
    "        * If you have many features $n$ but fewer number of examples $m$, the good solution to reduce **overfitting** is by reducing number of features $n$.\n",
    "\n",
    "3. Perform **Regularization**.\n",
    "    * This is very ussefull technic for training models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Idea Behind Regularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let say we have $n$ number of features, $n = 100$.\n",
    "    * $w_{0}, w_{1}, w_{2},\\; \\cdots\\; w_{99},\\; b$ - parameters\n",
    "\n",
    "        \n",
    "         If $w_{0} \\; \\cdots\\; w_{99},\\; b\\; $ will be **smaller** \n",
    "\n",
    "\n",
    "        Then\n",
    "\n",
    "\n",
    "        Our model will be equivalent to a **Simpler model**\n",
    "        \n",
    "\n",
    "        Therefore: our model will be **less** likely to **overfit**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So, to get **smaller** $w_{0}, w_{1}, w_{2}, \\cdots, w_{99},\\; b$\n",
    "\n",
    "    We will pinalize all $w_{j}$ by adding $\\frac{\\lambda}{2m}\\sum_{n=0}^{n-1}{w_{j}^2}$\n",
    "\n",
    "    Where\n",
    "\n",
    "    $\\lambda\\; \\; \\text{is regularization parameter}, \\lambda > 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "* Cost.\n",
    "    * Cost functions differ significantly between Linear and Logistic Regression, but adding Regularization to the equations is the same.\n",
    "* Gradient.\n",
    "    * The gradient functions for Linear and Logistic Regression are very similar. They differ only in the implementation of $f_{w,b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $f_{w,b}(\\mathbf{x}^{(i)}) = w_{j} \\cdot \\mathbf{x}_{j}^{(i)} + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $J(w,b) = \\frac{1}{2m} \\sum_{i=0}^{m-1}{(f_{w,b}(\\mathbf{x^{(i)}}) - y^{(i)})^{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "    \\text{repeat until convergence:} \\; \\; \\lbrace \\\\\n",
    "    \\; \\; \\; w_{j} = w_{j} - \\alpha \\frac{\\partial J(w,b)}{\\partial w_{j}} \\; \\; \\text{for  j = 0, \\dots, n-1} \\\\ \\\\\n",
    "    \\; \\; \\; b  =  b - \\alpha \\frac{\\partial J(w,b)}{\\partial b} \\\\ \\\\\n",
    "    \\\\ \\rbrace\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Where:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_{j}} = \\frac{1}{m}\\sum_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m}\\sum_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularized Linear Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f_{w,b}(\\mathbf{x}^{(i)}) = w_{j} \\cdot \\mathbf{x}_{j}^{(i)} + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$J(w,b) = \\frac{1}{2m} \\sum_{i=0}^{m-1}{(f_{w,b}(\\mathbf{x}^{(i)}) - y^{(i)})^{2}} + \\frac{\\lambda}{2m}\\sum_{j=0}^{n-1}w_{j}^{2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\text{repeat until convergence:} \\; \\lbrace \\\\\n",
    " \\; \\; \\;w_j = w_j - \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}\\;  \\text{for j := 0 \\dots n-1} \\\\\n",
    " \\; \\; \\; \\;b = b - \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n",
    "\\rbrace\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Where:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_{j}} = \\frac{1}{m}\\sum_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} + \\frac{\\lambda}{m}\\sum_{j=0}^{n-1}w^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m}\\sum_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "* By addind regularization term, we pinalize values of $w_j$ to be smaller.\n",
    "\n",
    "* If $\\lambda = 0$ means no regularization applied, hence model will **underfit**.\n",
    "\n",
    "* If $\\lambda\\;  \\text{is very large}$, model will **overfit**.\n",
    "\n",
    "* If $\\lambda$ is not too **small** and not too **large** but just **right** then our model will **generalized**.\n",
    "\n",
    "* The parameter $b$ is not regularized. This is standard practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow is an implementation of equations (1) adn (2), Note that this uses a _*standard pattern for this course,*_ a ***for loop*** over all ***m*** examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularized Linear Cost Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$J(w,b) = \\frac{1}{2m} \\sum_{i=0}^{m-1}{(f_{w,b}(\\mathbf{x}^{(i)}) - y^{(i)})^{2}} + \\frac{\\lambda}{2m}\\sum_{j=0}^{n-1}w_{j}^{2}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_linear_reg(X, y, w, b, lambda_):\n",
    "    \"\"\"\n",
    "    computes the cost over all examples\n",
    "\n",
    "    Args:\n",
    "        X (ndarray (m,n)): Data, m examples with n features\n",
    "        y (ndarray (n,)): target values\n",
    "        w (ndarray (n,)): model parameters\n",
    "        b (scalar)      : model parameter\n",
    "        lambda (scalar) : controls amount of regularization\n",
    "    Returns:\n",
    "        total_cost (scalar): cost\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    cost = 0.\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i], w) + b\n",
    "        cost += (f_wb_i - y[i])**2\n",
    "    cost /=(2*m)\n",
    "\n",
    "    reg_cost=0\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j]**2)\n",
    "    reg_cost = (lambda_/(2*m))*reg_cost\n",
    "    total_cost = cost + reg_cost\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python main function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C main function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized cost: 0.0791723937669179\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    np.random.seed(1)\n",
    "    X_tmp=np.array([[4.17022005e-01, 7.20324493e-01, 1.14374817e-04, 3.02332573e-01, 1.46755891e-01, 9.23385948e-02],\n",
    "                  [1.86260211e-01, 3.45560727e-01, 3.96767474e-01, 5.38816734e-01, 4.19194514e-01, 6.85219500e-01],\n",
    "                  [2.04452250e-01, 8.78117436e-01, 2.73875932e-02, 6.70467510e-01, 4.17304802e-01, 5.58689828e-01],\n",
    "                  [1.40386939e-01, 1.98101489e-01, 8.00744569e-01, 9.68261576e-01, 3.13424178e-01, 6.92322616e-01],\n",
    "                  [8.76389152e-01, 8.94606664e-01, 8.50442114e-02, 3.90547832e-02, 1.69830420e-01, 8.78142503e-01]])   \n",
    "    y_tmp = np.array([0,1,0,1,0])\n",
    "    w_tmp = np.array([-0.40165317, -0.07889237,  0.45788953,  0.03316528,  0.19187711, -0.18448437])\n",
    "    b_tmp = 0.5\n",
    "    lambda_tmp = 0.7\n",
    "    cost = compute_cost_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "    print(\"Regularized cost:\", cost)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularized Linear gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_{j}} = \\frac{1}{m}\\sum_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} + \\frac{\\lambda}{m}\\sum_{j=0}^{n-1}w^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m}\\sum_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_linear_reg(X, y, w, b, lambda_):\n",
    "    \"\"\"\n",
    "    Compute the gradient for linear regression\n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "\n",
    "    m,n =X.shape\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb=np.dot(X[i],w) + b\n",
    "        err=f_wb - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + (err * X[i, j])\n",
    "        dj_db = dj_db + err\n",
    "    dj_dw =dj_dw/ m\n",
    "    dj_db =dj_db/ m\n",
    "\n",
    "    for j in range(n):\n",
    "        dj_dw[j] =dj_dw[j] + ((lambda_/m)*w[j])\n",
    "\n",
    "    return  dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized Parameters:\n",
      "\n",
      "dj_dw[0]:\t-0.04226595935184001\n",
      "dj_dw[1]:\t0.05237246827859569\n",
      "dj_dw[2]:\t-0.00827465637378165\n",
      "dj_dw[3]:\t-0.024143317470786088\n",
      "dj_dw[4]:\t0.012555864679079941\n",
      "dj_dw[5]:\t-0.07695476271064926\n",
      "dj_db:\t-0.008768830987243648\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    X_tmp=np.array([[4.17022005e-01, 7.20324493e-01, 1.14374817e-04, 3.02332573e-01, 1.46755891e-01, 9.23385948e-02],\n",
    "                  [1.86260211e-01, 3.45560727e-01, 3.96767474e-01, 5.38816734e-01, 4.19194514e-01, 6.85219500e-01],\n",
    "                  [2.04452250e-01, 8.78117436e-01, 2.73875932e-02, 6.70467510e-01, 4.17304802e-01, 5.58689828e-01],\n",
    "                  [1.40386939e-01, 1.98101489e-01, 8.00744569e-01, 9.68261576e-01, 3.13424178e-01, 6.92322616e-01],\n",
    "                  [8.76389152e-01, 8.94606664e-01, 8.50442114e-02, 3.90547832e-02, 1.69830420e-01, 8.78142503e-01]])   \n",
    "    y_tmp = np.array([0,1,0,1,0])\n",
    "    w_tmp = np.array([-0.40165317, -0.07889237,  0.45788953,  0.03316528,  0.19187711, -0.18448437])\n",
    "    b_tmp = 0.5\n",
    "    lambda_tmp = 0.7\n",
    "\n",
    "    m,n = X_tmp.shape\n",
    "\n",
    "    dj_db_tmp, dj_dw_tmp =  compute_gradient_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "    print(\"Regularized Parameters:\\n\")\n",
    "    for i in range(n):\n",
    "        print(f\"dj_dw[{i}]:\\t{dj_dw_tmp[i]}\")\n",
    "    \n",
    "    print(f\"dj_db:\\t{dj_db_tmp}\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regressiom**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $f_{w,b}(\\mathbf{x}^{(i)}) = g(w_{j} \\cdot \\mathbf{x}_{j}^{(i)} + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $J(w,b) = \\frac{1}{2m} \\sum_{i=0}^{m-1}{(f_{w,b}(\\mathbf{x^{(i)}}) - y^{(i)})^{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "    \\text{repeat until convergence:} \\; \\; \\lbrace \\\\\n",
    "    \\; \\; \\; w_{j} = w_{j} - \\alpha \\frac{\\partial J(w,b)}{\\partial w_{j}} \\; \\; \\text{for  j = 0, \\dots, n-1} \\\\ \\\\\n",
    "    \\; \\; \\; b  =  b - \\alpha \\frac{\\partial J(w,b)}{\\partial b} \\\\ \\\\\n",
    "    \\\\ \\rbrace\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Where:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = w_{j}\\cdot\\mathbf{x}_{j}^{(i)} + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g(z) = \\frac{1}{1 + e^{(-z)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f_{w,b}(\\mathbf{x}^{(i)}) = g(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_{j}} = \\frac{1}{m}\\sum_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m}\\sum_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularized Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $f_{w,b}(\\mathbf{x}^{(i)}) = g(w_{j} \\cdot \\mathbf{x}_{j}^{(i)} + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$J(w,b) = \\frac{1}{2m} \\sum_{i=0}^{m-1}{(f_{w,b}(\\mathbf{x}^{(i)}) - y^{(i)})^{2}} + \\frac{\\lambda}{2m}\\sum_{j=0}^{n-1}w_{j}^{2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "    \\text{repeat until convergence:} \\; \\; \\lbrace \\\\\n",
    "    \\; \\; \\; w_{j} = w_{j} - \\alpha \\frac{\\partial J(w,b)}{\\partial w_{j}} \\; \\; \\text{for  j = 0, \\dots, n-1} \\\\ \\\\\n",
    "    \\; \\; \\; b  =  b - \\alpha \\frac{\\partial J(w,b)}{\\partial b} \\\\ \\\\\n",
    "    \\\\ \\rbrace\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*where*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = w_{j}\\cdot\\mathbf{x}_{j}^{(i)} + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g(z) = \\frac{1}{1 + e^{(-z)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f_{w,b}(\\mathbf{x}^{(i)}) = g(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_{j}} = \\frac{1}{m}\\sum_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} + \\frac{\\lambda}{m}\\sum_{j=0}^{n-1}w^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m}\\sum_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sigmoid function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g(z) = \\frac{1}{1 + e^{(-z)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Copute sigmoid\n",
    "    Arg:\n",
    "        z (scalar): prediction\n",
    "\n",
    "    Return:\n",
    "        logistic prediction\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularized Cost Logistic Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(w,b) = \\frac{1}{2m} \\sum_{i=0}^{m-1}{(f_{w,b}(\\mathbf{x}^{(i)}) - y^{(i)})^{2}} + \\frac{\\lambda}{2m}\\sum_{j=0}^{n-1}w_{j}^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f_{w,b}(\\mathbf{x}^{(i)}) = g(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g(z) = \\frac{1}{1 + e^{(-z)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logistic_reg(X, y, w, b, lambda_):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "        X (ndarray (m,n): Data, m examples with n features\n",
    "        y (ndarray (m,)): target values\n",
    "        w (ndarray (n,)): model parameters  \n",
    "        b (scalar)      : model parameter\n",
    "        lambda_ (scalar): Controls amount of regularization\n",
    "    Returns:\n",
    "        total_cost (scalar):  cost \n",
    "    \"\"\"\n",
    "\n",
    "    m,n=X.shape\n",
    "    cost=0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb = np.dot(X[i],w) + b\n",
    "        f_wb_i = sigmoid(f_wb)\n",
    "        cost += -y[i]*np.log(f_wb_i) - (1-y[i])*(np.log(1-f_wb_i))\n",
    "    cost /=m\n",
    "\n",
    "    reg_cost=0\n",
    "    for i in range(n):\n",
    "        reg_cost += w[i]**2\n",
    "    reg_cost =(lambda_/(2*m)) * reg_cost\n",
    "\n",
    "    total_cost = cost + reg_cost\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized cost: 0.6865981362012243\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    X=np.array([[4.17022005e-01, 7.20324493e-01, 1.14374817e-04],\n",
    "                  [1.86260211e-01, 3.45560727e-01, 3.96767474e-01],\n",
    "                  [2.04452250e-01, 8.78117436e-01, 2.73875932e-02],\n",
    "                  [1.40386939e-01, 1.98101489e-01, 8.00744569e-01],\n",
    "                  [8.76389152e-01, 8.94606664e-01, 8.50442114e-02]]) \n",
    "    y = np.array([0,1,0,1,0])\n",
    "    w = np.array([-0.40165317, -0.07889237,  0.45788953])\n",
    "    b = 0.5\n",
    "    lambda_ = 0.7\n",
    "\n",
    "    cost_ = compute_cost_logistic_reg(X, y, w, b, lambda_)\n",
    "\n",
    "    print(\"Regularized cost:\", cost_)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularized Gradient Logistic Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_{j}} = \\frac{1}{m}\\sum_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} + \\frac{\\lambda}{m}\\sum_{j=0}^{n-1}w^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m}\\sum_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f_{w,b}(\\mathbf{x}^{(i)}) = g(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g(z) = \\frac{1}{1 + e^{(-z)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_logistic_reg(X, y, w, b, lambda_):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns\n",
    "      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. w. \n",
    "      dj_db (scalar)            : The gradient of the cost w.r.t. b. \n",
    "    \"\"\"\n",
    "\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)\n",
    "        err_i = f_wb_i - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + (err_i*X[i][j])\n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m\n",
    "    dj_db = dj_db/m\n",
    "\n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m)*w[j]\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized Parameters: \n",
      "\n",
      "dj_dw[0]:\t0.08590186822598606\n",
      "dj_dw[1]:\t0.2318715168852964\n",
      "dj_dw[2]:\t-0.0019798089387517287\n",
      "dj_db: 0.20333487598147518\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    X=np.array([[4.17022005e-01, 7.20324493e-01, 1.14374817e-04],\n",
    "                  [1.86260211e-01, 3.45560727e-01, 3.96767474e-01],\n",
    "                  [2.04452250e-01, 8.78117436e-01, 2.73875932e-02],\n",
    "                  [1.40386939e-01, 1.98101489e-01, 8.00744569e-01],\n",
    "                  [8.76389152e-01, 8.94606664e-01, 8.50442114e-02]]) \n",
    "    y = np.array([0,1,0,1,0])\n",
    "    w = np.array([-0.40165317, -0.07889237,  0.45788953])\n",
    "    b = 0.5\n",
    "    lambda_ = 0.7\n",
    "    dj_db, dj_dw =  compute_gradient_logistic_reg(X, y, w, b, lambda_)\n",
    "\n",
    "    m,n = X.shape\n",
    "\n",
    "    print(\"Regularized Parameters: \\n\")\n",
    "    for i in range(n):\n",
    "        print(f\"dj_dw[{i}]:\\t{dj_dw[i]}\")\n",
    "\n",
    "    print(f\"dj_db: {dj_db}\", )\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
